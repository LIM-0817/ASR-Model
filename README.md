# End-to-End Automatic Speech Recognition (ASR) Model

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/PyTorch-1.13%2B-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white"/>
  <img src="https://img.shields.io/badge/WandB-Experiment-orange?style=for-the-badge&logo=weightsandbiases&logoColor=black"/>
</div>

<br>

**Listen, Attend and Spell (LAS)** ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜
Carnegie Mellon Univ. (CMU) 11-785 Deep Learning ê°•ì¢Œì˜ HW4P2 êµ¬ì¡°ë¥¼ ì‹œì‘ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸°ë²•ìœ¼ë¡œ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë¦¼.

## Performance Improvement

ìˆ˜ì‹­ ë²ˆì˜ ì‹¤í—˜ê³¼ êµ¬ì¡° ê°œì„ ì„ í†µí•´ ì´ˆê¸° ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

| Decoding Strategy | Metric (Levenshtein Distance) | Improvement |
| :--- | :---: | :--- |
| **Baseline (Greedy)** | 23.xx | - |
| **Final (Beam Search)** | **18.xx (Public) / 21.xx (Private)** | **â–¼ Performance Boost** |

## Key Improvements 

ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ë²•ë“¤ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©í–ˆìŠµë‹ˆë‹¤.

### 1. Architecture Enhancements
- **PBLSTM (Pyramidal Bi-LSTM)**: ì‹œê°„ ì°¨ì›ì„ ì••ì¶•í•˜ì—¬ ê¸´ ì‹œí€€ìŠ¤ í•™ìŠµ íš¨ìœ¨ ì¦ëŒ€
- **Add one more layer of PBLSTM**: ì¸ì½”ë”ì˜ ê¹Šì´ë¥¼ ëŠ˜ë ¤ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ëŠ¥ë ¥ ê°•í™”
- **Conv1d Stride Tuning**

### 2. Training Strategy
- **Scaling Factor**: ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤í­ë°œ ë°©ì§€
- **Staged Teacher Forcing Ratio**: 2 staged teacher forcing decayë¥¼ ì´ìš©
- **Spec Augmentation**: Time Masking, Frequency Maskingì„ í†µí•œ ë°ì´í„° ì¦ê°•

### 3. Inference & Attention
- **Attention Padding Masking**
- **Beam Search Implementation**: ë‹¨ìˆœ Greedy Decoding ëŒ€ì‹  testì‹œ beam searchì´ìš©í•´ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê°ì†Œ

## Visualization & Analysis

### 1. Attention Map Analysis


| Epoch 1 (Initial) | Epoch 150 (Converged) |
| :---: | :---: |
| <img width="100%" src="https://github.com/user-attachments/assets/fe2a73ce-19a4-40ee-9003-cc01b9e38298" /> | <img width="100%" src="https://github.com/user-attachments/assets/ed7ccb1d-269b-4ecf-9b0e-e1976f29300c" /> |
| í•™ìŠµ ì´ˆê¸°: ì •ë ¬ì´ í˜•ì„±ë˜ì§€ ì•ŠìŒ | **í•™ìŠµ ì™„ë£Œ: ì„ ëª…í•œ ëŒ€ê°ì„ (Diagonal) í˜•íƒœì˜<br>Alignmentê°€ í˜•ì„±ë¨ì„ í™•ì¸** |

### 2. Training Log and(Wandb)
**Best Model í›ˆë ¨ ë¡œê·¸**
<img width="100%" alt="Best Model Log" src="https://github.com/user-attachments/assets/40c80e61-a906-4865-aa04-7c70f7a518f3" />

<details>
<summary><strong>ğŸ“‚ Click to see All Experiments History (Hyperparameter Tuning)</strong></summary>
<br>
ëª¨ë“  run
<br><br>
<img width="100%" alt="All Run Log" src="https://github.com/user-attachments/assets/f80bc842-c170-480d-817a-67204283a658" />
</details>

## Installation & Usage

### 1. Requirements
```bash
pip install -r requirements.txt
```

### 2. dataset download
!! kaggle ê³„ì •ì—ì„œ API ìƒì„± í›„ .kaggle í´ë”ì— ì—…ë¡œë“œ í•„ìš” !!

```bash
# 1. Kaggle API ì„¤ì¹˜
pip install -q kaggle

# 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
kaggle competitions download -c attention-based-speech-recognition -p ./data

# 3. ì••ì¶• í•´ì œ
unzip -q ./data/attention-based-speech-recognition.zip -d ./data
```

### 3. Train
config.yaml íŒŒì¼ì—ì„œ hyperparameter tuning ì´í›„ í•™ìŠµì„ ì§„í–‰(config.pyëŠ” ê±´ë“œë¦¬ì§€ ë§ê¸°!)
```bash
python train.py
```

### 4. Test(Inference)
```bash
python test.py
```

## Project Structure
```
.
â”œâ”€â”€ models/             # Listener, Speller, Attention modules
â”œâ”€â”€ utils/              # Helpers, Metrics, Visualization
â”œâ”€â”€ config.yaml         # Hyperparameter configuration
â”œâ”€â”€ train.py            # Training script
â”œâ”€â”€ test.py             # Inference script (Beam Search included)
â”œâ”€â”€ dataset.py
â””â”€â”€ README.md
```